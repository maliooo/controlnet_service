{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 512)\n"
     ]
    }
   ],
   "source": [
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置代理，端口7890\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "image_path = \"/workspace/测试文件夹/bedroom.jpg\"\n",
    "image = Image.open(image_path)\n",
    "# 按照短边缩放到512，长边等比例缩放, 同时长边可以被8整除\n",
    "width, height = image.size\n",
    "if width < height:\n",
    "    new_width = 512\n",
    "    new_height = int(height * (512 / width)) // 8 * 8\n",
    "    image = image.resize((new_width, new_height))\n",
    "else:\n",
    "    new_height = 512\n",
    "    new_width = int(width * (512 / height)) // 8 * 8\n",
    "    image = image.resize((new_width, new_height))\n",
    "\n",
    "print(image.size)\n",
    "# image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `OneFormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading a single model for all three tasks\n",
    "# 加载本地模型\n",
    "model_path = r\"/home/public/ai_chat_data/models/oneformer_ade20k_swin_large\"\n",
    "processor = OneFormerProcessor.from_pretrained(model_path,local_files_only=True)\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5,  ..., 0, 0, 0],\n",
       "        [5, 5, 5,  ..., 0, 0, 0],\n",
       "        [5, 5, 5,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [3, 3, 3,  ..., 3, 3, 3],\n",
       "        [3, 3, 3,  ..., 3, 3, 3],\n",
       "        [3, 3, 3,  ..., 3, 3, 3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Semantic Segmentation\n",
    "with torch.no_grad():\n",
    "    semantic_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\")\n",
    "    semantic_outputs = model(**semantic_inputs)\n",
    "    # pass through image_processor for postprocessing\n",
    "    predicted_semantic_map = processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\n",
    "    predicted_semantic_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instance Segmentation\n",
    "instance_inputs = processor(images=image, task_inputs=[\"instance\"], return_tensors=\"pt\")\n",
    "instance_outputs = model(**instance_inputs)\n",
    "# pass through image_processor for postprocessing\n",
    "predicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\n",
    "\n",
    "# Panoptic Segmentation\n",
    "panoptic_inputs = processor(images=image, task_inputs=[\"panoptic\"], return_tensors=\"pt\")\n",
    "panoptic_outputs = model(**panoptic_inputs)\n",
    "# pass through image_processor for postprocessing\n",
    "predicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `OneFormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 512)\n",
      "dict_keys(['pixel_values', 'pixel_mask', 'task_inputs'])\n",
      "torch.Size([1, 3, 640, 853])\n",
      "odict_keys(['class_queries_logits', 'masks_queries_logits', 'auxiliary_predictions', 'encoder_hidden_states', 'pixel_decoder_hidden_states', 'transformer_decoder_hidden_states', 'transformer_decoder_object_queries', 'transformer_decoder_contrastive_queries', 'transformer_decoder_mask_predictions', 'transformer_decoder_class_predictions', 'transformer_decoder_auxiliary_predictions', 'task_token', 'attentions'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# load OneFormer fine-tuned on ADE20k for universal segmentation\n",
    "processor = OneFormerProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "url = (\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    ")\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(image.size)\n",
    "\n",
    "# Semantic Segmentation\n",
    "inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
    "print(inputs.keys())\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)  #\n",
    "\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for semantic postprocessing\n",
    "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[(image.height, image.width)]\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [9, 9, 9,  ..., 6, 6, 6],\n",
       "        [9, 9, 9,  ..., 6, 6, 6],\n",
       "        [9, 9, 9,  ..., 6, 6, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_semantic_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
